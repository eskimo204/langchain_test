from google.colab import drive
drive.mount('/content/drive')

! pip install -U langchain openai chromadb langchain-experimental # 최신 버전이 필요합니다 (멀티 모달을 위해)
! pip install "unstructured[all-docs]" pillow pydantic lxml pillow matplotlib chromadb tiktoken

# 파일 경로, 파일 이름 설정
fpath = "/content/drive/MyDrive/cj"
fname = "cj.pdf"

import os
from langchain_text_splitters import CharacterTextSplitter
from unstructured.partition.pdf import partition_pdf

extract_images_in_pdf=True
!pip install pytesseract

!apt install tesseract-ocr
!apt-get install -y poppler-utils

import pytesseract

import os  # 파일 경로를 다루기 위한 모듈

from unstructured.partition.pdf import partition_pdf  # PDF 파일에서 요소를 추출하기 위한 함수
from unstructured.documents.elements import Table, CompositeElement  # 테이블 및 텍스트 요소의 타입 확인을 위한 클래스
from langchain.text_splitter import CharacterTextSplitter  # 텍스트를 분할하기 위한 클래스

# 필요한 경우, Tiktoken을 사용하여 텍스트를 토큰화하기 위한 임포트
import tiktoken
# 텍스트에서 문서 추출
import pytesseract
# Tesseract 경로 설정 (필요시)
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'  # Windows 예시

# PDF 파일에서 이미지, 테이블, 텍스트 조각을 추출
# unstructured 라이브러리의 partition_pdf 함수를 사용
def extract_pdf_elements(path, fname):
    """
    PDF 파일에서 이미지, 테이블, 그리고 텍스트 조각을 추출합니다.
    path: 이미지(.jpg)를 저장할 파일 경로
    fname: 파일 이름
    """
    return partition_pdf(
        filename=os.path.join(path, fname),
        extract_images_in_pdf=True,  # PDF 내 이미지 추출 활성화
        infer_table_structure=True,  # 테이블 구조 추론 활성화
        chunking_strategy="by_title",  # 제목별로 텍스트 조각화
        max_characters=4000,  # 최대 문자 수
        new_after_n_chars=3800,  # 이 문자 수 이후에 새로운 조각 생성
        combine_text_under_n_chars=2000,  # 이 문자 수 이하의 텍스트는 결합
        image_output_dir_path=path,  # 이미지 출력 디렉토리 경로
    )


# PDF에서 추출된 요소들을 테이블과 텍스트로 분류하는 categorize_elements 함수
# unstructured 라이브러리에서 추출된 요소들을 입력으로 받아, 이를 각각 테이블과 텍스트 리스트로 분류
def categorize_elements(raw_pdf_elements):
    """
    PDF에서 추출된 요소를 테이블과 텍스트로 분류합니다.
    raw_pdf_elements: unstructured.documents.elements의 리스트
    """
    tables = []  # 테이블 저장 리스트
    texts = []  # 텍스트 저장 리스트
    for element in raw_pdf_elements:
        if "unstructured.documents.elements.Table" in str(type(element)):
            tables.append(str(element))  # 테이블 요소 추가
        elif "unstructured.documents.elements.CompositeElement" in str(type(element)):
            texts.append(str(element))  # 텍스트 요소 추가
    return texts, tables


# PDF 파일의 요소들을 추출
raw_pdf_elements = extract_pdf_elements(fpath, fname)

# 추출된 요소들에서 텍스트와 테이블을 분류
texts, tables = categorize_elements(raw_pdf_elements)

# 추출된 텍스트들을 특정 크기의 토큰으로 분할
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=4000, chunk_overlap=0  # 텍스트를 4000 토큰 크기로 분할, 중복 없음
)
joined_texts = " ".join(texts)  # 텍스트 결합
texts_4k_token = text_splitter.split_text(joined_texts)  # 분할 실행

# texts_4k_token의 모든 내용을 출력하는 함수
def print_texts_4k_token(texts_4k_token):
    """
    텍스트 조각 리스트의 모든 내용을 출력합니다.
    texts_4k_token: 텍스트 조각 리스트
    """
    # 리스트의 각 텍스트 조각을 순회하면서 출력
    for i, text_chunk in enumerate(texts_4k_token):
        print(f"--- Chunk {i + 1} ---")
        print(text_chunk)
        print("\n")  # 조각 간의 구분을 위해 줄 바꿈 추가

# texts_4k_token의 모든 내용을 출력
print_texts_4k_token(texts_4k_token)

pip install langchain-openai

import openai
os.environ["OPENAI_API_KEY"] = '공개되면 안되는 정보'

 # LangChain 라이브러리에서 제공하는 클래스 중 하나로, 주어진 입력을 문자열 형식으로 파싱(데이터를 다루기 쉬운 형태로 변환)하는 역할
 # 모델의 출력이나 기타 데이터를 문자열로 변환
from langchain_core.output_parsers import StrOutputParser
# 대화형 프롬프트를 생성하고 관리하는 데 사용되는 템플릿 클래스
# 대화형 AI 모델과의 상호작용을 위한 프롬프트를 쉽게 구성할 수 있다.
from langchain_core.prompts import ChatPromptTemplate
# OpenAI의 대화형 모델(GPT-3 등)과 상호작용하기 위한 클래스
# 클래스는 OpenAI API를 사용하여 대화형 응답을 생성하고, 이를 통해 사용자와의 대화 인터페이스를 제공
from langchain_openai import ChatOpenAI

# 텍스트 요소의 요약 생성


def generate_text_summaries(texts, tables, summarize_texts=False):
    """
    텍스트 요소 요약
    texts: 문자열 리스트
    tables: 문자열 리스트
    summarize_texts: 텍스트 요약 여부를 결정. True/False
    """

    # 프롬프트 설정
    # 요약 작업에 사용할 프롬프트 텍스트를 정의
    # 이 프롬프트는 텍스트와 테이블을 요약하는 역할
    prompt_text = """You are an assistant tasked with summarizing tables and text for retrieval. \
    These summaries will be embedded and used to retrieve the raw text or table elements. \
    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} """
    # 프롬프트 생성
    prompt = ChatPromptTemplate.from_template(prompt_text)

    # 텍스트 요약 체인
    # OpenAI의 GPT-4 모델을 사용하여 요약을 생성
    model = ChatOpenAI(temperature=0, model="gpt-4")
    # summarize_chain은 요약 작업을 수행하는 체인
    # 이 체인은 입력 요소를 받아 프롬프트를 적용하고, 모델을 통해 요약을 생성한 후, 이를 문자열로 파싱
    summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()

    # 요약을 위한 빈 리스트 초기화
    text_summaries = []
    table_summaries = []

    # 제공된 텍스트에 대해 요약이 요청되었을 경우 적용
    if texts and summarize_texts:
        text_summaries = summarize_chain.batch(texts, {"max_concurrency": 5})
    elif texts:
        text_summaries = texts

    # 제공된 테이블에 적용
    if tables:
        table_summaries = summarize_chain.batch(tables, {"max_concurrency": 5})

    return text_summaries, table_summaries


# 텍스트, 테이블 요약 가져오기
text_summaries, table_summaries = generate_text_summaries(
    texts_4k_token, tables, summarize_texts=True
)

import base64
# base64 모듈은 바이너리 데이터(사용자가 사용한 정보나 숫자 값을 그대로 저장하는 데이터)를 ASCII 문자로 인코딩하고 디코딩하는 방법을 제공
# 바이너리 데이터를 텍스트 형식으로 변환할 때 사용
import os
# os 모듈은 운영 체제와 상호작용하는 다양한 기능을 제공
# 파일 및 디렉토리 조작, 환경 변수 접근 등을 할 수 있다.

from langchain_core.messages import HumanMessage


def encode_image(image_path):
    # 이미지 파일을 base64 문자열로 인코딩합니다.
    with open(image_path, "rb") as image_file:
    # with 문을 사용하여 파일을 엽니다. 이는 파일을 열고 자동으로 닫아주는 안전한 파일 작업 방법
        return base64.b64encode(image_file.read()).decode("utf-8")
        # image_file.read()는 파일의 모든 내용을 읽어옵니다. 이 내용은 바이너리 데이터로 읽혀진다.


# image_summarize는 주어진 이미지와 프롬프트를 기반으로 이미지 요약을 생성하는 기능
def image_summarize(img_base64, prompt):
    # 이미지 요약을 생성합니다.
    # ChatOpenAI 객체 생성
    chat = ChatOpenAI(model="gpt-4o", max_tokens=2048)

    # chat.invoke([...])는 ChatOpenAI 객체를 통해 대화를 시작하고 결과를 가져오는 메서드
    msg = chat.invoke(
        [
            HumanMessage(
                # content=[...]는 메시지의 내용을 나타내는 리스트
                content=[
                    # 사용자가 입력한 프롬프트 전달
                    {"type": "text", "text": prompt},
                    { # 이미지 URL 형식의 메시지. base64로 인코딩된 이미지 데이터를 포함
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"},
                    },
                ]
            )
        ]
    )
    return msg.content

# 특정 디렉토리에서 추출된 .jpg 이미지 파일들에 대해 요약과 base64 인코딩된 문자열을 생성하는 기능
def generate_img_summaries(path):
    """
    이미지에 대한 요약과 base64 인코딩된 문자열을 생성합니다.
    path: Unstructured에 의해 추출된 .jpg 파일 목록의 경로
    """

    # base64로 인코딩된 이미지를 저장할 리스트
    img_base64_list = []

    # 이미지 요약을 저장할 리스트
    image_summaries = []

    # 요약을 위한 프롬프트(이미지 요약을 위한 프롬프트)
    prompt = """You are an assistant tasked with summarizing images for retrieval. \
    These summaries will be embedded and used to retrieve the raw image. \
    Give a concise summary of the image that is well optimized for retrieval."""

    # 이미지에 적용하는 작업
    for img_file in sorted(os.listdir(path)):
        if img_file.endswith(".jpg"):
            img_path = os.path.join(path, img_file)
            base64_image = encode_image(img_path)
            img_base64_list.append(base64_image)
            image_summaries.append(image_summarize(base64_image, prompt))

    return img_base64_list, image_summaries


# 이미지 요약 실행
img_base64_list, image_summaries = generate_img_summaries(fpath)

# uuid 모듈은 UUID(Universally Unique Identifier)를 생성하고 관리하는 데 사용
import uuid


# MultiVectorRetriever는 LangChain 라이브러리에서 제공하는 다중 벡터 검색을 수행하는 클래스
from langchain.retrievers.multi_vector import MultiVectorRetriever
# LangChain에서 제공하는 메모리 기반의 데이터 저장소
from langchain.storage import InMemoryStore
# Chroma는 LangChain 커뮤니티에서 제공하는 벡터 스토어
from langchain_community.vectorstores import Chroma
# LangChain Core에서 제공하는 문서 객체입니다. 문서를 생성하고 관리하는 데 사용
from langchain_core.documents import Document
# OpenAI에서 제공하는 임베딩 기능을 LangChain에서 사용할 수 있도록 지원하는 클래스입니다. 텍스트나 문장을 임베딩하는 데에 활용
from langchain_openai import OpenAIEmbeddings


def create_multi_vector_retriever(
    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images
):
    """
    요약을 색인화하지만 원본 이미지나 텍스트를 반환하는 검색기를 생성합니다.
    """

    # 저장 계층 초기화
    store = InMemoryStore()
    id_key = "doc_id"

    # 멀티 벡터 검색기 생성
    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore=store,
        id_key=id_key,
    )

    # 문서를 벡터 저장소와 문서 저장소에 추가하는 헬퍼 함수
    def add_documents(retriever, doc_summaries, doc_contents):
        global documentt_ids
        doc_ids = [
            str(uuid.uuid4()) for _ in doc_contents
        ]  # 문서 내용마다 고유 ID 생성

        summary_docs = [
            Document(page_content=s, metadata={id_key: doc_ids[i]})
            for i, s in enumerate(doc_summaries)
        ]
        retriever.vectorstore.add_documents(
            summary_docs
        )  # 요약 문서를 벡터 저장소에 추가
        retriever.docstore.mset(
            list(zip(doc_ids, doc_contents))
        )  # 문서 내용을 문서 저장소에 추가

    # 텍스트, 테이블, 이미지 추가
    if text_summaries:
        add_documents(retriever, text_summaries, texts)

    if table_summaries:
        add_documents(retriever, table_summaries, tables)

    if image_summaries:
        add_documents(retriever, image_summaries, images)



    # Mission
    # store에 저장 내용 출력(보기)하기
    # 벡터 저장소에 저장된 내용 출력(보기)하기
    # for, while 또는, vectorstore, docstore 레프런스 이용. retriever 레프런스 이용.


    # 2024 07 20 새벽에 작성
    # 일단 컴퓨터 구조에 대해서는 진짜 자세하게 공부함, 그리고 이런 이해를 바탕으로 컴퓨터의 전원이 들어왔을 때 부팅이 어떻게 되는지, 프로그램이 어떻게 실행되는지까지 자세하게 공부했음
    # 아무리 찾아봐도 store에 저장 내용을 출력하거나 벡터 저장소에 있는 내용을 통째로 출력하는 방법은 존재하지 않음....(아마 이런 메서드가 특벽히 없어서 그런거 같음)
    # 구글링을 해서 다른 개발자의 글을 읽어보든, 유듀브를 찾아보든, 심지어 직접적으로 챗지피치에게 사정사정 물어봐도 방법이 안나옴
    # 마지막으로 문서 내용마다 고유키가 발급되는데, 이 키에 해당하는 문서 내용을 출력하려고 해봤지만 오류 발생(-> 이거 작성하고 코랩이 아예 멈춰서 처음부터 재부팅)

    # 그래도 그 전에 진행했던, 어떠한 질문, 쿼리를 받고, 그에 해당하는 내용을 출력하는 것은 가능해보임
    # 여기서 걱정되는 점은, 전에 진행했던 벡터 저장소는 chromaDB가 아닌 FAISS였다는 점(추가적인 공부 필요), 그리고 이 코드에서는 쿼리에 따른 내용을 벡터 저장소 뿐만이 아니라 문서 저장소에서도 찾아야한다는 점 2가지의 문제가 있다.

    # 추가적으로 코드를 작성할려고 했지만, 코랩이 자꾸 먹통인 관계로 일단 내일 오전에 다시 시작해봐야겠음


    return retriever

# 요약을 색인화하기 위해 사용할 벡터 저장소
vectorstore = Chroma(
    collection_name="sample-rag-multi-modal", embedding_function=OpenAIEmbeddings()
)

# 검색기 생성
retriever_multi_vector_img = create_multi_vector_retriever(
    vectorstore,
    text_summaries,
    texts,
    table_summaries,
    tables,
    image_summaries,
    img_base64_list,
)

import io
import re

from IPython.display import HTML, display
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from PIL import Image


def plt_img_base64(img_base64):
    """base64 인코딩된 문자열을 이미지로 표시"""
    # base64 문자열을 소스로 사용하는 HTML img 태그 생성
    image_html = f'<img src="data:image/jpeg;base64,{img_base64}" />'
    # HTML을 렌더링하여 이미지 표시
    display(HTML(image_html))


def looks_like_base64(sb):
    """문자열이 base64로 보이는지 확인"""
    return re.match("^[A-Za-z0-9+/]+[=]{0,2}$", sb) is not None


def is_image_data(b64data):
    """
    base64 데이터가 이미지인지 시작 부분을 보고 확인
    """
    image_signatures = {
        b"\xff\xd8\xff": "jpg",
        b"\x89\x50\x4e\x47\x0d\x0a\x1a\x0a": "png",
        b"\x47\x49\x46\x38": "gif",
        b"\x52\x49\x46\x46": "webp",
    }
    try:
        header = base64.b64decode(b64data)[:8]  # 처음 8바이트를 디코드하여 가져옴
        for sig, format in image_signatures.items():
            if header.startswith(sig):
                return True
        return False
    except Exception:
        return False


def resize_base64_image(base64_string, size=(128, 128)):
    """
    Base64 문자열로 인코딩된 이미지의 크기 조정
    """
    # Base64 문자열 디코드
    img_data = base64.b64decode(base64_string)
    img = Image.open(io.BytesIO(img_data))

    # 이미지 크기 조정
    resized_img = img.resize(size, Image.LANCZOS)

    # 조정된 이미지를 바이트 버퍼에 저장
    buffered = io.BytesIO()
    resized_img.save(buffered, format=img.format)

    # 조정된 이미지를 Base64로 인코딩
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def split_image_text_types(docs):
    """
    base64로 인코딩된 이미지와 텍스트 분리
    """
    b64_images = []
    texts = []
    for doc in docs:
        # 문서가 Document 타입인 경우 page_content 추출
        if isinstance(doc, Document):
            doc = doc.page_content
        if looks_like_base64(doc) and is_image_data(doc):
            doc = resize_base64_image(doc, size=(1300, 600))
            b64_images.append(doc)
        else:
            texts.append(doc)
    return {"images": b64_images, "texts": texts}


def img_prompt_func(data_dict):
    """
    컨텍스트를 단일 문자열로 결합
    """
    formatted_texts = "\n".join(data_dict["context"]["texts"])
    messages = []

    # 이미지가 있으면 메시지에 추가
    if data_dict["context"]["images"]:
        for image in data_dict["context"]["images"]:
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image}"},
            }
            messages.append(image_message)

    # 분석을 위한 텍스트 추가
    text_message = {
        "type": "text",
        "text": (
            "You are financial analyst tasking with providing investment advice.\n"
            "You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\n"
            "Use this information to provide investment advice related to the user question. Answer in Korean. Do NOT translate company names.\n"
            f"User-provided question: {data_dict['question']}\n\n"
            "Text and / or tables:\n"
            f"{formatted_texts}"
        ),
    }
    messages.append(text_message)
    return [HumanMessage(content=messages)]


def multi_modal_rag_chain(retriever):
    """
    멀티모달 RAG 체인
    """

    # 멀티모달 LLM
    model = ChatOpenAI(temperature=0, model="gpt-4o", max_tokens=2048)

    # RAG 파이프라인
    chain = (
        {
            "context": retriever | RunnableLambda(split_image_text_types),
            "question": RunnablePassthrough(),
        }
        | RunnableLambda(img_prompt_func)
        | model
        | StrOutputParser()
    )

    return chain


# RAG 체인 생성
chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)

# 검색 질의 실행
query = "EV/NTM 및 NTM 매출 성장률을 기준으로 흥미로운 투자처인 기업 이름을 알려주세요. EV/NTM 멀티플과 과거를 고려하시나요?"

# 질의에 대한 문서 6개를 검색합니다.
docs = retriever_multi_vector_img.invoke(query, limit=6)

# 문서의 개수 확인
len(docs)  # 검색된 문서의 개수를 반환합니다.

# 검색 결과 확인
query = "Mongo DB, Cloudflare, Datadog 의 EV/NTM 및 NTM rev 성장률은 얼마인가요?"
docs = retriever_multi_vector_img.invoke(query, limit=6)

# 문서의 개수 확인
len(docs)

# 관련 이미지를 반환합니다.
plt_img_base64(docs[0])

# img_base64_list 리스트의 20번 index 이미지를 base64 형식으로 표시합니다.
plt_img_base64(img_base64_list[3])

image_summaries[3]

# RAG 체인 실행
print(chain_multimodal_rag.invoke(query))
